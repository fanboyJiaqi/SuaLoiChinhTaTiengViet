{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w3z3cEFZhQ1",
        "outputId": "5b9064fc-65e3-4e93-94b8-7ef3fa9e6cb9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV7gnnYnZMns"
      },
      "source": [
        "#1.THU THẬP DỮ LIỆU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "znTxFbf0YjM0"
      },
      "outputs": [],
      "source": [
        "class FileData(object):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        with open(path, encoding='utf-16') as f:\n",
        "          self.data = f.read()\n",
        "          #print(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "62R5ekxBZoQR"
      },
      "outputs": [],
      "source": [
        "ABSOLUTE_PATH = r\"E:\\Python\\language\\Train_Full\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-lyYbU9AR_AO"
      },
      "outputs": [],
      "source": [
        "c_tri =  \"\\Chinh tri Xa hoi\"\n",
        "\n",
        "d_song = \"\\Doi song\"\n",
        "\n",
        "khoa_hoc = \"\\Khoa hoc\"\n",
        "\n",
        "kinh_doanh = \"\\Kinh doanh\"\n",
        "\n",
        "p_luat = \"\\Phap luat\"\n",
        "\n",
        "suc_khoe = \"\\Suc khoe\"\n",
        "\n",
        "the_gioi = \"\\The gioi\"\n",
        "\n",
        "the_thao = \"\\The thao\"\n",
        "\n",
        "van_hoa = \"\\Van hoa\"\n",
        "\n",
        "vi_tinh = \"\\Vi tinh\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "c_tri =  \"/Chinh tri Xa hoi\"\n",
        "\n",
        "#d_song = \"/Doi song\"\n",
        "\n",
        "khoa_hoc = \"/Khoa hoc\"\n",
        "\n",
        "kinh_doanh = \"/Kinh doanh\"\n",
        "\n",
        "p_luat = \"/Phap luat\"\n",
        "\n",
        "suc_khoe = \"/Suc khoe\"\n",
        "\n",
        "#the_gioi = \"/The gioi\"\n",
        "\n",
        "the_thao = \"/The thao\"\n",
        "\n",
        "#van_hoa = \"/Van hoa\"\n",
        "\n",
        "#vi_tinh = \"/Vi tinh\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_ebuOhjqdR6A"
      },
      "outputs": [],
      "source": [
        "#corpus = [c_tri, d_song, khoa_hoc, kinh_doanh, p_luat, suc_khoe, the_gioi, the_thao, van_hoa, vi_tinh]\n",
        "corpus = [c_tri, khoa_hoc, kinh_doanh, p_luat, suc_khoe, the_thao]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VdxyVSPJZ1eK"
      },
      "outputs": [],
      "source": [
        "for folder_path in range(len(corpus)):\n",
        "    corpus[folder_path] = ABSOLUTE_PATH + corpus[folder_path]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w88ltME4SiP",
        "outputId": "72ea8bcd-0bcc-45c6-8f9f-b7a94be6438c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'E:\\\\Python\\\\language\\\\Train_Full/Kinh doanh'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "La-aEcptdPgo"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "file_list = []\n",
        "#count = 0\n",
        "\n",
        "for folder_path in corpus:\n",
        "    count = 0\n",
        "    for name in os.listdir(folder_path):\n",
        "        count +=1\n",
        "        if count == 1500:\n",
        "          break        \n",
        "        path = os.path.join(folder_path, name)\n",
        "        if not os.path.isfile(path):\n",
        "            continue\n",
        "        file = FileData(path)\n",
        "        file_list.append( file.data )\n",
        "        # count +=1\n",
        "        # if count == 5:\n",
        "        #   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bpdg-jn87df",
        "outputId": "5a8fc0bd-dbd9-4651-b6a8-e769f8a7450d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8994"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(file_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lưu dữ liệu dưới dạng file pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzFpHKHaP0l2",
        "outputId": "bdb890f8-3f94-4fe2-ec97-9e12b790af16"
      },
      "outputs": [],
      "source": [
        "#path_corpus = \"E:\\Python\\language\\input_corpus.pkl\"\n",
        "path_corpus = 'E:\\Python\\language\\input_cutdown_corpus.pkl'\n",
        "\n",
        "with open(path_corpus, 'wb') as pickle_file:\n",
        "    pickle.dump(file_list, pickle_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMAmC-ktKreu"
      },
      "source": [
        "# 2.IMPORT DU LIEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mRph_nIndXi9"
      },
      "outputs": [],
      "source": [
        "pk_path = '/home/phuong/Documents/2023_NLP/Code/PreData/input_cutdown_corpus.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Dr2HuIqQBIB"
      },
      "outputs": [],
      "source": [
        "with open(pk_path, \"rb\") as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-jXs1dPCRcPd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8994"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YidyP3TqVxKu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Xây dựng trạm dừng nghỉ cho xe đò\\nTrên diện tích 10.000m2 sẽ xây dựng khu nhà ăn tự chọn phục vụ 400-500 người, khu siêu thị, trạm y tế, điện thoại công cộng, hai khu vực nhà vệ sinh, trạm xăng dầu... (ảnh mô hình). Dự kiến tổng kinh phí đầu tư khoảng 5 tỉ đồng, công trình sẽ thi công và đưa vào sử dụng trong năm 2005. \\n\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXUdGdxKxw3"
      },
      "source": [
        "### 2.1 TIEN XU LY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tPo_wynOJgtM"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i9f62EMn7CLV"
      },
      "outputs": [],
      "source": [
        "alphabet = '^[ _abcdefghijklmnopqrstuvwxyz0123456789áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ!\\\"\\',\\-\\.:;?_\\(\\)]+$'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unakwWmbLmrN"
      },
      "source": [
        "Xây dựng bộ ngữ liệu là 1 list chứa các câu. Thay thế dấu cách xuống dòng bằng dấu chấm cuối câu. Từ dấu chấm đó, tách văn bản theo các câu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HOhfO5RzV1GW"
      },
      "outputs": [],
      "source": [
        "def latin_extract(data):\n",
        "\n",
        "    # extract Latin- characters only\n",
        "   \n",
        "    latin_extract_data=[]\n",
        "    # duyet qua tung van ban\n",
        "    for i in data:\n",
        "      if i == 1:\n",
        "        break\n",
        "      # thay the xuong dong la dau cham ket thuc\n",
        "      i=i.replace(\"\\n\",\".\")\n",
        "      # tach van ban theo dau cham ket thuc\n",
        "      sentences=i.split(\".\")\n",
        "      for j in sentences:\n",
        "          #print(\"j hien tai = \", j)\n",
        "          #print( \"split=\", j.split() )\n",
        "          if len(j.split()) > 2 and re.match(alphabet, j.lower()):\n",
        "            \n",
        "              latin_extract_data.append(j)\n",
        "              #print(\"j moi = \",j)\n",
        "\n",
        "    return latin_extract_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D7_YqN_DJYUA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137028\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' Từ 2-12: cấp giấy chứng nhận quyền sử dụng đất theo mẫu mới'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data = latin_extract(data)\n",
        "\n",
        "print(len(training_data))\n",
        "training_data[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Sp6NYXjjPPm8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đông đảo người dân đến vui chơi tại Thảo Cầm Viên - Ảnh: T\n",
            "Các cổ động viên Bảo vệ thực vật Sài Gòn - Dofilm và nông nghiệp Sài Gòn Pakse đang chào đón cuộc đua - Ảnh: Nguyên Khôi \n",
            "Lễ mít tinh kỷ niệm 31 năm ngày giải phóng miền Nam, thống nhất đất nước, ngày Quốc tế Lao động 1-5 và chào mừng Đại hội X của Đảng thành công đã được tổ chức long trọng vào sáng 27-4\n",
            "Ngày 28-4, Chương trình nghệ thuật đặc biệt \"Cảm xúc tháng tư\" do Sở VH-TT TP\n",
            "HCM phối hợp với Đài truyền hình VN tổ chức tại công viên 30-4 thu hút đông đảo người dân đến xem\n",
            " Anh Nguyễn Hồng (Quận Bình Thạnh) chia sẻ \"3 năm liền đều ra đây xem Cúp truyền hình rồi, chỉ để xem được mấy giây tay đua nào cán đích nhưng tôi đem sẵn cả nước, báo và ổ bánh mì ra đây\"\n",
            "Giữa dòng người, hai bạn trẻ Thùy, Hạnh vẫn thoăn thoắt giới thiệu sản phẩm và bán nước Trà xanh không độ\n",
            " \"Hôm nay không khí đông vui quá, hàng bán chạy hẳn, ráng xong ngày hôm nay thôi, nhóm mình sẽ đi chơi vào ngày mai\"\n",
            "Peter và Anna đến từ Đan Mạch không ngừng chụp hình không khí tại công viên\n",
            " Anna đang làm việc tại Việt Nam, còn Peter chỉ là khách du lịch bị Anna \"dụ dỗ\" sang VN chơi lễ\n"
          ]
        }
      ],
      "source": [
        "i = 100\n",
        "while i < 110:\n",
        "  print(training_data[i])\n",
        "  i += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Liệt các từ, các kí tự gõ sai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Nw2dp5NzJpp5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o6t2kr8pai6l"
      },
      "outputs": [],
      "source": [
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "letters2=list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n",
        "\"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n",
        "\"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n",
        "\"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n",
        "\"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n",
        "\"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n",
        "\"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n",
        "\"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n",
        "\"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n",
        "\"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n",
        "\n",
        "# dia phuong\n",
        "region={\"ẻ\":\"ẽ\",\"ẽ\":\"ẻ\",\"ũ\":\"ủ\",\"ủ\":\"ũ\",\"ã\":\"ả\",\"ả\":\"ã\",\"ỏ\":\"õ\",\"õ\":\"ỏ\",\"i\":\"j\"}\n",
        "region2={\"s\":\"x\",\"l\":\"n\",\"n\":\"l\",\"x\":\"s\",\"d\":\"gi\",\"S\":\"X\",\"L\":\"N\",\"N\":\"L\",\"X\":\"S\",\"Gi\":\"D\",\"D\":\"Gi\"}\n",
        "\n",
        "# nguyen am\n",
        "vowel=list(\"aeiouyáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵ\")\n",
        "\n",
        "# viet tat\n",
        "acronym={\"không\":\"ko\",\" anh\":\" a\",\"em\":\"e\",\"biết\":\"bít\",\"giờ\":\"h\",\"gì\":\"j\",\"muốn\":\"mún\",\"học\":\"hok\",\"yêu\":\"iu\",\n",
        "         \"chồng\":\"ck\",\"vợ\":\"vk\",\" ông\":\" ô\",\"được\":\"đc\",\"tôi\":\"t\",\n",
        "         \"Không\":\"Ko\",\" Anh\":\" A\",\"Em\":\"E\",\"Biết\":\"Bít\",\"Giờ\":\"H\",\"Gì\":\"J\",\"Muốn\":\"Mún\",\"Học\":\"Hok\",\"Yêu\":\"Iu\",\n",
        "         \"Chồng\":\"Ck\",\"Vợ\":\"Vk\",\" Ông\":\" Ô\",\"Được\":\"Đc\",\"Tôi\":\"T\",}\n",
        "\n",
        "# teencode\n",
        "teen={\"ch\":\"ck\",\"ph\":\"f\",\"th\":\"tk\",\"nh\":\"nk\",\n",
        "      \"Ch\":\"Ck\",\"Ph\":\"F\",\"Th\":\"Tk\",\"Nh\":\"Nk\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hàm giả lập teencode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Pob_Cy9oarAs"
      },
      "outputs": [],
      "source": [
        "# function for adding mistake( noise)\n",
        "def teen_code(sentence,pivot):\n",
        "    random = np.random.uniform(0,1,1)[0]\n",
        "    new_sentence=str(sentence)\n",
        "    if random>pivot:\n",
        "        for word in acronym.keys():\n",
        "            if re.search(word, new_sentence):\n",
        "                random2 = np.random.uniform(0,1,1)[0]\n",
        "                if random2 <0.5:\n",
        "                    new_sentence=new_sentence.replace(word,acronym[word])\n",
        "        for word in teen.keys(): \n",
        "            if re.search(word, new_sentence):\n",
        "                random3 = np.random.uniform(0,1,1)[0]\n",
        "                if random3 <0.05:\n",
        "                    new_sentence=new_sentence.replace(word,teen[word])        \n",
        "        return new_sentence\n",
        "    else:\n",
        "        return sentence\n",
        "    \n",
        "\n",
        "def add_noise(sentence, pivot1,pivot2):\n",
        "    sentence=teen_code(sentence,0.5)\n",
        "    noisy_sentence = \"\"\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        if sentence[i] not in letters:\n",
        "            noisy_sentence+=sentence[i]\n",
        "        else: \n",
        "            random = np.random.uniform(0,1,1)[0]   \n",
        "            if random < pivot1:\n",
        "                noisy_sentence+=(sentence[i])\n",
        "            elif random<pivot2:\n",
        "                if sentence[i] in typo.keys() and sentence[i] in region.keys():\n",
        "                    random2=np.random.uniform(0,1,1)[0]\n",
        "                    if random2<=0.4:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random2<0.8:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random2<0.95 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in typo.keys():\n",
        "                    random3=np.random.uniform(0,1,1)[0]\n",
        "                    if random3<=0.6:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random3<0.9 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])                        \n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in region.keys():\n",
        "                    random4=np.random.uniform(0,1,1)[0]\n",
        "                    if random4<=0.6:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random4<0.85 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])                        \n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif i<len(sentence)-1 :\n",
        "                    if sentence[i] in region2.keys() and (i==0 or sentence[i-1] not in letters) and sentence[i+1] in vowel:\n",
        "                        random5=np.random.uniform(0,1,1)[0]\n",
        "                        if random5<=0.9:\n",
        "                            noisy_sentence+=region2[sentence[i]]\n",
        "                        else:\n",
        "                            noisy_sentence+=sentence[i]\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "\n",
        "            else:\n",
        "                new_random = np.random.uniform(0,1,1)[0]\n",
        "                if new_random <=0.33:\n",
        "                    if i == (len(sentence) - 1):\n",
        "                        continue\n",
        "                    else:\n",
        "                        noisy_sentence+=(sentence[i+1])\n",
        "                        noisy_sentence+=(sentence[i])\n",
        "                        i += 1\n",
        "                elif new_random <= 0.66:\n",
        "                    random_letter = np.random.choice(letters2, 1)[0]\n",
        "                    noisy_sentence+=random_letter\n",
        "                else:\n",
        "                    pass\n",
        "      \n",
        "        i += 1\n",
        "    return noisy_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc bjệt thì xóa chúng đi\n",
            "Làm sach văn bản, tách các cụm tw trog câu, nế uc k ítự đgc biệt tì xóa chúng đi\n",
            "Làm sich văn bản, tách các cụm ừt trno gnâu, nếp có kí to đặc biệt thì xóa chúng iđ\n"
          ]
        }
      ],
      "source": [
        "print(add_noise('Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc biệt thì xóa chúng đi',0.94,0.985))\n",
        "print(add_noise('Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc biệt thì xóa chúng đi',0.85,0.88))\n",
        "print(add_noise('Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc biệt thì xóa chúng đi',0.8,0.9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QToUfBLCdRd8"
      },
      "source": [
        "Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc biệt thì xóa chúng đi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wb4qlCgYym0r"
      },
      "outputs": [],
      "source": [
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "g61rIjlnauFP"
      },
      "outputs": [],
      "source": [
        "def extract_phrases(text):\n",
        "    return re.findall(r'\\w[\\w ]+', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cgvHdn9Na_7I"
      },
      "outputs": [],
      "source": [
        "def _extract_phrases(data):\n",
        "    phrases = itertools.chain.from_iterable(extract_phrases(text) for text in data)\n",
        "    phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n",
        "\n",
        "    return phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YXFni6LLy5W_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334297\n",
            "2 triệu con gia cầm của 12 huyện thị\n"
          ]
        }
      ],
      "source": [
        "phrases = _extract_phrases(training_data)\n",
        "\n",
        "print(len(phrases))\n",
        "print(phrases[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXnuL9DJfPDs"
      },
      "source": [
        "Chia tập ngữ điệu thành 2-gram. Từng phần tử trong list gồm có 2 từ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2m2uUHJQ6ZdT"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "T11TNxY_6xde"
      },
      "outputs": [],
      "source": [
        "# Một từ tiếng Việt không có quá 7 kí tự, một bigram lúc này không quá 15 kí tự\n",
        "NGRAM = 2 \n",
        "MAXLEN = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SyWAljsu2t7b"
      },
      "outputs": [],
      "source": [
        "def gen_ngrams(words, n=2):\n",
        "    return ngrams(words.split(), n)\n",
        "\n",
        "def generate_bi_grams(phrases):   \n",
        "    list_ngrams = []\n",
        "    for p in tqdm(phrases):\n",
        "  \n",
        "      # neu khong nham trong bang chu cai thi bo qua\n",
        "      if not re.match(alphabet, p.lower()):\n",
        "        continue\n",
        "\n",
        "      # tach p thanh cac bi gram   \n",
        "      for ngr in gen_ngrams(p, NGRAM):\n",
        "        if len(\" \".join(ngr)) < MAXLEN:\n",
        "          list_ngrams.append(\" \".join(ngr))\n",
        "\n",
        "    return list_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HwTIxtQs21Im"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/334297 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 334297/334297 [00:01<00:00, 218458.93it/s]\n"
          ]
        }
      ],
      "source": [
        "list_ngrams = generate_bi_grams(phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l7O_Ht4F7yum"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2981627\n",
            "triệu gia\n"
          ]
        }
      ],
      "source": [
        "print(len(list_ngrams))\n",
        "print(list_ngrams[7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtBfrCAjh8s8"
      },
      "source": [
        "Mã hóa câu thành ma trận 2 chiều và giải mã câu đó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uX8jC5BRe8uy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
            "199\n"
          ]
        }
      ],
      "source": [
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "print(alphabet)\n",
        "print(len(alphabet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aNZKniR1fhZj"
      },
      "outputs": [],
      "source": [
        "# Padding hai đầu đoạn văn bản để tất cả bigram có độ dài bằng nhau\n",
        "def encoder_data(text, maxlen=MAXLEN):\n",
        "        #print(\"Maxlen\", maxlen)\n",
        "        text = \"\\x00\" + text\n",
        "        #print(\"text\", text)\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        #print(\"X ban dau\", x)\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "          for j in range(i+1, maxlen):\n",
        "            x[j, 0] = 1\n",
        "        return x\n",
        "      \n",
        "def decoder_data(x):\n",
        "    x = x.argmax(axis=-1)\n",
        "    #print(\"x hien tai\", x)\n",
        "    dem = ''.join(alphabet[i] for i in x)\n",
        "    #print(\"Do dai cau van\", len(dem))\n",
        "\n",
        "    return dem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LC_A8rc6gKwC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40, 199)\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(encoder_data(\"Tôi tên là LÊ TUẤN\").shape)\n",
        "print(encoder_data(\"Tôi tên là LÊ TUẤN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXt3D_tbisN2"
      },
      "source": [
        "#3.BUILD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "43MbypBriuwJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-07 09:12:27.996293: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-12-07 09:12:27.998449: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-07 09:12:28.026942: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-07 09:12:28.027016: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-07 09:12:28.027043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-07 09:12:28.034726: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-07 09:12:28.035089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-07 09:12:28.966837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Build the neural network\n",
        "# this is adapted from the seq2seq architecture, which can be used for Machine Translation, Text Summarization Image Captioning ...\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, TimeDistributed, Dense,LSTM, Bidirectional\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam # - Works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "n-IeuGbS5vOl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MAXLEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "URGzW2go5wTn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(alphabet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PPyg_Q2siyJr"
      },
      "outputs": [],
      "source": [
        "encoder = LSTM(256,input_shape=(MAXLEN, len(alphabet)), return_sequences=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_zhW5WUyjEPn"
      },
      "outputs": [],
      "source": [
        "decoder=Bidirectional(LSTM(256, return_sequences=True, dropout=0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zIrnqBxKjGU6"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(encoder)\n",
        "model.add(decoder)\n",
        "model.add(TimeDistributed(Dense(256)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(TimeDistributed(Dense(len(alphabet))))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2uL7KFl_jIgb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 40, 256)           466944    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 40, 512)           1050624   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 40, 256)           131328    \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 256)           0         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 40, 199)           51143     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 40, 199)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1700039 (6.49 MB)\n",
            "Trainable params: 1700039 (6.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EoK3qdpujKqQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "w5Wj2G2gjOak"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, valid_data = train_test_split(list_ngrams, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ZFoVo_yDjvIQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'triệu gia'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list_ngrams)\n",
        "list_ngrams[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "w94uZcE_jvwK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2981627\n",
            "2385301\n",
            "596326\n",
            "thuyền có\n"
          ]
        }
      ],
      "source": [
        "print(len(list_ngrams))\n",
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(train_data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gzirJBY5kqpX"
      },
      "outputs": [],
      "source": [
        "# we have to use data- generation medthod cause this dataset is too large to fit into memory\n",
        "BATCH_SIZE = 512\n",
        "def generate_data(data, batch_size):\n",
        "    cur_index = 0\n",
        "    while True:\n",
        "        x, y = [], []\n",
        "        for i in range(batch_size):  \n",
        "            y.append(encoder_data(data[cur_index]))\n",
        "            x.append(encoder_data(add_noise(data[cur_index],0.94,0.985)))\n",
        "            cur_index += 1\n",
        "            if cur_index > len(data)-1:\n",
        "                cur_index = 0\n",
        "        yield np.array(x), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4_t-VOdWk4PI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n",
            "Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):  # Generate 5 batches for demonstration\n",
        "    batch_x, batch_y = next(train_generator)\n",
        "    print(\"Batch X shape:\", batch_x)\n",
        "    print(\"Batch Y shape:\", batch_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "I-9-dLlLk6Ca"
      },
      "outputs": [],
      "source": [
        "# train the model and save to the Model folder\n",
        "checkpointer = ModelCheckpoint(filepath='/home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5', save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "FkqPYVcGlc1z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "4658/4658 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9782\n",
            "Epoch 1: val_loss improved from inf to 0.02529, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n",
            "4658/4658 [==============================] - 5512s 1s/step - loss: 0.1030 - accuracy: 0.9782 - val_loss: 0.0253 - val_accuracy: 0.9940\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/phuong/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4658/4658 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9946\n",
            "Epoch 2: val_loss improved from 0.02529 to 0.01831, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n",
            "4658/4658 [==============================] - 6245s 1s/step - loss: 0.0225 - accuracy: 0.9946 - val_loss: 0.0183 - val_accuracy: 0.9955\n",
            "Epoch 3/5\n",
            "4658/4658 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9956\n",
            "Epoch 3: val_loss improved from 0.01831 to 0.01600, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n",
            "4658/4658 [==============================] - 6282s 1s/step - loss: 0.0176 - accuracy: 0.9956 - val_loss: 0.0160 - val_accuracy: 0.9960\n",
            "Epoch 4/5\n",
            "4658/4658 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9961\n",
            "Epoch 4: val_loss improved from 0.01600 to 0.01427, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n",
            "4658/4658 [==============================] - 6273s 1s/step - loss: 0.0155 - accuracy: 0.9961 - val_loss: 0.0143 - val_accuracy: 0.9963\n",
            "Epoch 5/5\n",
            "4658/4658 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9963\n",
            "Epoch 5: val_loss improved from 0.01427 to 0.01307, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n",
            "4658/4658 [==============================] - 6266s 1s/step - loss: 0.0143 - accuracy: 0.9963 - val_loss: 0.0131 - val_accuracy: 0.9966\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ff9540a6010>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit( train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=5,\n",
        "                    validation_data=validation_generator, validation_steps=len(valid_data)//BATCH_SIZE,\n",
        "                    callbacks=[checkpointer] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRewGYVd-GIJ"
      },
      "source": [
        "#5.DỰ ĐOÁN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LOYH9uWF-OAf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-14 13:09:17.343935: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-12-14 13:09:17.409811: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-14 13:09:17.683112: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-14 13:09:17.683150: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-14 13:09:17.684881: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-14 13:09:17.826721: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-14 13:09:17.828032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-14 13:09:18.993434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from keras.models import load_model\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams,word_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "import unidecode\n",
        "import string\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zajwNyjRAVtK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7f7da8677050>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model = load_model('E:\\Python\\language\\spell_0.99.h5')\n",
        "model = load_model('/home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5')\n",
        "model.make_predict_function()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-XjsEp02AduP"
      },
      "outputs": [],
      "source": [
        "NGRAM=2\n",
        "MAXLEN=40 # INPUT SHAPE\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "accepted_char=list((string.digits + ''.join(letters)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JCLDKMzPl19O"
      },
      "outputs": [],
      "source": [
        "def call(sentence):\n",
        "    def extract_phrases(text):\n",
        "        pattern = r'\\w[\\w ]*|\\s\\W+|\\W+'\n",
        "        return re.findall(pattern, text)\n",
        "\n",
        "    def encoder_data(text, maxlen=MAXLEN):\n",
        "            text = \"\\x00\" + text\n",
        "            x = np.zeros((maxlen, len(alphabet)))\n",
        "            for i, c in enumerate(text[:maxlen]):\n",
        "                x[i, alphabet.index(c)] = 1\n",
        "            if i < maxlen - 1:\n",
        "              for j in range(i+1, maxlen):\n",
        "                x[j, 0] = 1\n",
        "            return x    \n",
        "            \n",
        "    def decoder_data(x):\n",
        "        x = x.argmax(axis=-1)\n",
        "        return ''.join(alphabet[i] for i in x)\n",
        "\n",
        "    def nltk_ngrams(words, n=2):\n",
        "        return ngrams(words.split(), n)\n",
        "\n",
        "    def guess(ngram):\n",
        "        text = ' '.join(ngram)\n",
        "        preds = model.predict(np.array([encoder_data(text)]), verbose=0)\n",
        "        return decoder_data(preds[0]).strip('\\x00')\n",
        "\n",
        "    def correct(sentence):\n",
        "        for i in sentence:\n",
        "            if i not in accepted_char:\n",
        "                sentence=sentence.replace(i,\" \")\n",
        "        ngrams = list(nltk_ngrams(sentence, n=NGRAM))\n",
        "        guessed_ngrams = list(guess(ngram) for ngram in ngrams)\n",
        "\n",
        "        print(\"N gram\", ngrams)\n",
        "        print(\"guess\", guessed_ngrams)\n",
        "\n",
        "        #return guessed_ngrams\n",
        "\n",
        "        candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
        "        for nid, ngram in (enumerate(guessed_ngrams)):\n",
        "            for wid, word in (enumerate(re.split(' +', ngram))):\n",
        "                candidates[nid + wid].update([word])\n",
        "\n",
        "        # for c in candidates:\n",
        "        #     print(c.most_common(1))        \n",
        "        output = ' '.join(c.most_common(1)[0][0] for c in candidates)  \n",
        "        return output\n",
        "\n",
        "    guess = correct(sentence)\n",
        "\n",
        "    return guess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uEibQsCG-aLS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N gram [('Các', 'phát'), ('phát', 'thank'), ('thank', 'viên'), ('viên', 'ngưowfi'), ('ngưowfi', 'dẫn'), ('dẫn', 'chương'), ('chương', 'trìnk'), ('trìnk', 'đeu'), ('đeu', 'là'), ('là', 'cac'), ('cac', 'chuyeen'), ('chuyeen', 'gja'), ('gja', 'sử'), ('sử', 'dụng'), ('dụng', 'ngôn'), ('ngôn', 'ngữ'), ('ngữ', 'Thế'), ('Thế', 'nkưng'), ('nkưng', 'đa'), ('đa', 'phần'), ('phần', 'họ'), ('họ', 'ềđu'), ('ềđu', 'cho'), ('cho', 'rằng'), ('rằng', 'mìnk'), ('mìnk', 'ko'), ('ko', 'có'), ('có', 'khiếu'), ('khiếu', 'ăn'), ('ăn', 'nosi'), ('nosi', 'từ'), ('từ', 'nkỏ'), ('nkỏ', 'vậy'), ('vậy', 'tạo'), ('tạo', 'sao'), ('sao', 'hoj'), ('hoj', 'vẫn'), ('vẫn', 'thafnh'), ('thafnh', 'công'), ('công', 'nkờ'), ('nkờ', 'vào'), ('vào', 'tài'), ('tài', 'ăn'), ('ăn', 'nói'), ('nói', 'của'), ('của', 'mìnk'), ('mìnk', 'Nguyên'), ('Nguyên', 'nkân'), ('nkân', 'rất'), ('rất', 'đơn'), ('đơn', 'giản'), ('giản', 'đó'), ('đó', 'là'), ('là', 'vì'), ('vì', 'họ'), ('họ', 'tự'), ('tự', 'nkận'), ('nkận', 'thấy'), ('thấy', 'mìnk'), ('mìnk', 'nói'), ('nói', 'năng'), ('năng', 'ko'), ('ko', 'tốt'), ('tốt', 'nên'), ('nên', 'luôn'), ('luôn', 'cố'), ('cố', 'gắng'), ('gắng', 'để'), ('để', 'nâng'), ('nâng', 'cao'), ('cao', 'kĩ'), ('kĩ', 'năng'), ('năng', 'giao'), ('giao', 'tiế')]\n",
            "guess ['Các phát', 'phát thanh', 'thanh viên', 'viên người', 'người dẫn', 'dẫn chương', 'chương trình', 'trình đều', 'đều là', 'là các', 'các chuyên', 'chuyên gia', 'gia sử', 'sử dụng', 'dụng ngôn', 'ngôn ngữ', 'ngữ Thế', 'Thế nhưng', 'nhưng đa', 'đa phần', 'phần họ', 'họ đều', 'đều cho', 'cho rằng', 'rằng mình', 'mình không', 'không có', 'có khiếu', 'khiếu ăn', 'ăn nói', 'nói từ', 'từ nhỏ', 'nhỏ vậy', 'vậy tạo', 'tạo sao', 'sao học', 'họ vẫn', 'vẫn thành', 'thành công', 'công nhờ', 'nhờ vào', 'vào tài', 'tài ăn', 'ăn nói', 'nói của', 'của mình', 'mình Nguyên', 'Nguyên nhân', 'nhân rất', 'rất đơn', 'đơn giản', 'giản đó', 'đó là', 'là vì', 'vì họ', 'họ tự', 'tự nhận', 'nhận thấy', 'thấy mình', 'mình nói', 'nói năng', 'năng không', 'không tốt', 'tốt nên', 'nên luôn', 'luôn cố', 'cố gắng', 'gắng để', 'để nâng', 'nâng cao', 'cao kĩ', 'kĩ năng', 'năng giao', 'giao tiếp']\n",
            "Các phát thank viên, ngưowfi dẫn chương trìnk đeu là cac chuyeen gja sử dụng ngôn ngữ. Thế nkưng đa phần họ ềđu cho rằng mìnk ko có khiếu ăn nosi từ nkỏ, vậy tạo sao hoj vẫn thafnh công nkờ vào tài ăn nói của mìnk? Nguyên nkân rất đơn giản, đó là vì họ tự nkận thấy mìnk nói năng ko tốt, nên luôn cố gắng để nâng cao kĩ năng giao tiế\n",
            "Các phát thanh viên người dẫn chương trình đều là các chuyên gia sử dụng ngôn ngữ Thế nhưng đa phần họ đều cho rằng mình không có khiếu ăn nói từ nhỏ vậy tạo sao học vẫn thành công nhờ vào tài ăn nói của mình Nguyên nhân rất đơn giản đó là vì họ tự nhận thấy mình nói năng không tốt nên luôn cố gắng để nâng cao kĩ năng giao tiếp\n"
          ]
        }
      ],
      "source": [
        "#sentence = \"Các phát thank viên dẫn cương trink\"\n",
        "#sentence = \"Hom lay tooi đi hoc\"\n",
        "#sentence ='Tôi yêu thích họk ngôn ngữ mớj'\n",
        "#sentence = 'Thnh pố Hồ Chí Mình là mộp địa điểm du lịch nổi tiếg'\n",
        "sentence = 'Các phát thank viên, ngưowfi dẫn chương trìnk đeu là cac chuyeen gja sử dụng ngôn ngữ. Thế nkưng đa phần họ ềđu cho rằng mìnk ko có khiếu ăn nosi từ nkỏ, vậy tạo sao hoj vẫn thafnh công nkờ vào tài ăn nói của mìnk? Nguyên nkân rất đơn giản, đó là vì họ tự nkận thấy mìnk nói năng ko tốt, nên luôn cố gắng để nâng cao kĩ năng giao tiế'\n",
        "guess = call(sentence)\n",
        "print(sentence)\n",
        "print(guess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N gram [('Các', 'phát'), ('phát', 'thank'), ('thank', 'viên'), ('viên', 'ngưowfi'), ('ngưowfi', 'dẫn'), ('dẫn', 'chương'), ('chương', 'trìnk'), ('trìnk', 'đeu'), ('đeu', 'là'), ('là', 'cac'), ('cac', 'chuyeen'), ('chuyeen', 'gja'), ('gja', 'sử'), ('sử', 'dụng'), ('dụng', 'ngôn'), ('ngôn', 'ngữ')]\n",
            "guess ['Các phát', 'phát thanh', 'thanh viên', 'viên người', 'người dẫn', 'dẫn chương', 'chương trình', 'trình đều', 'đều là', 'là các', 'các chuyên', 'chuyên gia', 'gia sử', 'sử dụng', 'dụng ngôn', 'ngôn ngữ']\n",
            "Các phát thank viên, ngưowfi dẫn chương trìnk đeu là cac chuyeen gja sử dụng ngôn ngữ.\n",
            "Các phát thanh viên người dẫn chương trình đều là các chuyên gia sử dụng ngôn ngữ\n"
          ]
        }
      ],
      "source": [
        "sentence = 'Các phát thank viên, ngưowfi dẫn chương trìnk đeu là cac chuyeen gja sử dụng ngôn ngữ.'\n",
        "guess = call(sentence)\n",
        "print(sentence)\n",
        "print(guess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/home/phuong/Documents/2023_NLP/Code/PreData/test_corpus.pkl', \"rb\") as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8994"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 762ms/step\n",
            "Tôi tên là Nguyễn_Tiến_Đạt , hiện là sinh_viên Đại_học CN_GTVT tại Hà_Nội .\n"
          ]
        }
      ],
      "source": [
        "import vietokenizer\n",
        "tokenizer= vietokenizer.vntokenizer()\n",
        "t = tokenizer('Tôi tên là Nguyễn Tiến Đạt, hiện là sinh viên Đại học CN GTVT tại Hà Nội.')\n",
        "print(t)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gV7gnnYnZMns",
        "GMAmC-ktKreu",
        "hEXUdGdxKxw3",
        "CXt3D_tbisN2",
        "g4oXdRcLuEgw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
